[{"id":"a99376fa230b8d8b33ec300ad3be8330","title":"What? Gemini demo video is a edited version","content":"Recently, Gemini’s debut has taken the tech world by storm, seemingly outshining GPT-4 and gaining widespread attention. Tech giant Google, the parent company of Alphabet (Nasdaq: GOOG), witnessed a 5.31% surge in its stock price on December 7th, reaching $136.93 per share. This marked its best performance since August 29th, pushing its total market value to $1.72 trillion. However, some analysts suspect that Google’s promotional materials for Gemini may have been exaggerated.\nAccording to Google’s released data, Gemini Ultra is touted as the first large model to outperform human experts in the Massively Multitask Language Understanding (MMLU) task, achieving an impressive score of 90.0%. In comparison, human experts scored 89.8%, and GPT-4 scored 86.4%. The MMLU dataset encompasses 57 subjects, including mathematics, physics, history, law, medicine, and ethics, testing the model’s knowledge base and problem-solving capabilities.\nHowever, only the Gemini Pro version has been publicly presented, while details about the Gemini Ultra version, such as parameters, remain undisclosed. This lack of transparency has led to a perception of both grandiosity and insecurity. Shortly after Gemini’s launch on December 6th, some netizens pointed out discrepancies in Google’s promotional materials. For instance, when Google claimed Gemini’s MMLU score surpassed GPT-4, stating GPT-4’s score was 86.4%, the 60-page technical report revealed a small footnote for Gemini Ultra’s MMLU test results, mentioning the use of the “CoT@32” prompt technique, attempting 32 times and selecting the best result. In contrast, GPT-4 was evaluated without prompt words, and under this standard, Gemini Ultra’s actual test result was 83.7%, lower than GPT-4’s 86.4%. As a result, fact-checking initiatives began soon after the release of the 60-page technical report.\n\nIn terms of multitasking comparison, pitting Gemini against GPT-4 few-shot results is an unfair assessment. It’s akin to giving Gemini a draft for a math problem, allowing it to use a calculator, while expecting GPT-4 to solve it mentally! Moreover, Gemini had 32 chances to answer, selecting the best one from 32 responses, thanks to CoT@32. If GPT-4 could speak, it might express frustration!\nWhat is the true capability of Gemini if it doesn’t cheat?\nHuggingFace’s Technical Lead, Philipp Schmid, created a new chart, revealing that with 5-shot testing, Gemini’s score is actually 83.7%, not 90.0%.\n\nIn the realm of video understanding, Gemini demonstrates impressive capabilities, as showcased in the smooth and captivating demo videos. However, the most astonishing real-time inference effects were achieved through post-editing—addressing concerns about the smoothness and other aspects. As public opinion intensified, Google admitted to the editing of the video. Originally intended as a grand unveiling, it turned into a situation where actions spoke louder than words.\nIn most benchmark tests, Gemini Ultra only slightly outperforms OpenAI’s GPT-4 model. In other words, Google’s best AI model made minor improvements on work completed by OpenAI at least a year ago. During the time Google has been striving to catch up with OpenAI, the latter has spent nearly a year developing its next-generation artificial intelligence model, GPT-5.\n","slug":"What-Gemini-demo-video-is-a-edited-version","date":"2023-12-12T07:11:16.000Z","categories_index":"News","tags_index":"IT news,IT comment,ITFisher.com,Gemini","author_index":"IT Fisher"},{"id":"bc878e746b2ddaee1cd1728c6e28409d","title":"AI, a wave that must be joined","content":"\nYesterday, Google’s big move arrived. On December 6th local time, Google CEO Sundar Pichai officially announced the launch of Gemini 1.0. I believe many people who are following closely have noticed this. When it comes to discussions on the Internet in China and abroad, it’s like two different circles. One key word is layoffs and overseas, while the other is AI.\nAI CompetitionIf you don’t pay special attention to AI, you may miss many key points. Many people think AI equals ChatGPT. What’s so interesting about a chat tool? Answering questions and searching are similar; in China, there’s Baidu’s Wenxin Yiyuan, Xiao Ai, or Siri.\nIf you break out of the information cocoon and look far and wide, you’ll find that things have changed. Now, in the text category, indeed, ChatGPT is dominant, but there are still heavyweight products in other categories. For example, in the image category, there’s Midjourney, which can generate various images based on keywords. Baidu and Alibaba have also launched similar products, such as Alibaba’s Tongyi Wanxiang, but the effects are indeed very different.\nIn the video category, there are products like Pika released last week and Adobe’s Firefly, which can generate interesting videos with just a keyword.\nAI has now expanded from pure text to images, models, and sound in all aspects. It’s not just ChatGPT dominating; it’s a comprehensive development.\nGoogle’s Big Model GeminiTurning back to this big model, the key is multimodal. After training, Gemini 1.0 can simultaneously recognize and understand text, images, audio, etc. Therefore, it can comprehensively understand the details of information in the input and answer questions related to complex topics. Therefore, it is particularly good at reasoning about complex subjects such as mathematics and physics.\nGoogle spared no effort this year, merging Google Brain and DeepMind, finally launching this most powerful and versatile model. There were rumors in March, and it entered the “coming soon” status at the I&#x2F;O conference in May. As the saying goes, good things come to those who wait. Let’s see in January whether it’s bragging or a celebration.\n","slug":"AI-a-wave-that-must-be-joined","date":"2023-12-11T04:11:58.000Z","categories_index":"AI","tags_index":"IT news,IT comment,ITFisher.com","author_index":"IT Fisher"}]